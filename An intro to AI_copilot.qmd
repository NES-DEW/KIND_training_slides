---
title: "AI and Co-Pilot"
subtitle: "A critical introduction for Public Health"
date: 2025-05-27
author: "[Brendan Clarke](mailto:brendan.clarke2@nhs.scot), NHS Education for Scotland"
date-format: DD/MM/YYYY
bibliography: references.bib
reference-section-title: Further reading
format:
    revealjs:
        theme: [default, custom.scss]
        width: 1600
        margin: 0.1
        logo: src/images/KLN_banner_v05_125_d.png
        css: src//images//logo_d.css
        incremental: true
        embed-resources: true
---

```{r}
#| echo: false
#| warning: false

knitr::opts_chunk$set(echo = F, warning = F, message = F, results = "asis", fig.width = 30, fig.height = 10, fig.align="center")

library(pacman)
p_load(tidyverse, KINDR)

colz <- c("#f29559", "#f2d492","#202c39", "#f4f3ee" )
theme_set(theme_classic())
theme_update(panel.background = element_rect(fill = "#202c39"),
        plot.background = element_rect(fill = "#202c39"),
        axis.line = element_line(size = 0.5, linetype = "solid",
                                   colour = "#f4f3ee"),
        axis.text = element_text(colour="#f4f3ee"),
        text = element_text(colour="#f4f3ee", size = 40),
        legend.background = element_rect(fill = "#4D5661"),
        strip.text = element_text(colour="#f4f3ee"),
        strip.background = element_rect(fill="#4D5661", color="#202c39"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        )
```

## Welcome
+ this session is üå∂: for beginners
+ several aims
1. Copilot is coming!
1. to suggest that the term AI is troublesome
1. to introduce some of the different technologies that get lumped together as AI
1. review salient criticisms and worries about AI
1. public health-focused examples: free text, repetitious tasks, redrafting

## The KIND network

-   a social learning space for staff working with **k**nowledge, **i**nformation, and **d**ata across health, social care, and housing in Scotland
-   we offer social support, free training, mentoring, community events, ...
-   [Teams channel](https://teams.microsoft.com/l/team/19%3AQZ7-PbFVcziG2piHLt1_ifey3I2cwFL0yBuTSS8vVao1%40thread.tacv2/conversations?groupId=106d08f3-9026-40e2-b3c7-87cd87304d58&tenantId=10efe0bd-a030-4bca-809c-b5e6745e499a) / [mailing list](https://forms.office.com/pages/responsepage.aspx?id=veDvEDCgykuAnLXmdF5JmpopIZB9ynRJnrPUHVFccipURjM2NkZJUkhGOFlQRjQxRFhVUTgwT0UwVyQlQCN0PWcu)

## Copilot

+ recent SDH&C network [meeting recording](https://scottish.sharepoint.com/sites/CCLG/Shared%20Documents/7.%20%F0%9F%99%85%F0%9F%8F%BEEnabling%20Technology/Recordings/Copilot%20and%20the%20new%20Microsoft365%20Contract.%20What%20benefits%20could%20it%20bring%20for%20NHS%20Scotland_-20250507_130017-Meeting%20Recording.mp4?web=1&referrer=Teams.TEAMS-ELECTRON&referrerScenario=MeetingChicletGetLink.view): tens of thousands of Copilot licences procured
+ embedded large-language model-based system
+ safeguards around NHS data


## A philosophical question
+ do submarines swim?
+ what's wrong with that question?
+ "artificial intelligence" or "artificial" + "intelligence"

## Hype

-   There's a *lot* of hype about AI at the moment (see [this graph](https://www.google.com/imgres?imgurl=https%3A%2F%2Fassets.bwbx.io%2Fimages%2Fusers%2FiqjWHBFdfxIU%2Fi8RUqd2T_1Bs%2Fv2%2FpidjEfPlU1QWZop3vfGKsrX.ke8XuWirGYh1PKgEw44kE%2F-1x-1.png&tbnid=TPmvfqSuBPNC2M&vet=12ahUKEwjw5fuQs4WHAxUkU6QEHZyfDdkQMygDegQIARBP..i&imgrefurl=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2024-06-06%2Fnvidia-microsoft-and-apple-are-bigger-than-china-s-stock-market&docid=beNq4RhizA7SbM&w=1200&h=675&q=nvidia%20microsoft%20graph&ved=2ahUKEwjw5fuQs4WHAxUkU6QEHZyfDdkQMygDegQIARBP))
-   Underneath the hype, there's a lot of genuinely exciting stuff going on too
-   That exciting stuff is likely to have some impact on health and care work

## Motive

1. The *intelligence* part of AI is misleading: *producing* vs *understanding*
1. AI covers many different technologies with different strengths/weaknesses
1. And everything is complicated by the hype </br> ![So hot right now screenshot from Zoolander](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRzbu0IXXvTWSW4Bim6PWKkjJQyn6PL7au6LQ&s){height="300px"}

## Producing and understanding: The Chinese room

@searle1980

> "Suppose that I'm locked in a room and given a large batch of Chinese writing. Suppose furthermore (as is indeed the case) that I know no Chinese, either written or spoken, and that I'm not even confident that I could recognize Chinese writing"

. . .


However, he is supplied with a set of intelligible rules for manipulating these Chinese symbols

. . .


+ "ÁÅ´" is the opposite of "Ê∞¥"
+ "ÂÖ≠" is more than "Âõõ"

## Question

Does this poor bloke locked in a room understand the Chinese symbols?

. . .

Now suppose that we start asking him questions (in English) where he has to pick between responses

. . .

+ Is "ÂÖ≠" more than "Âõõ"? If so, respond with "ÊòØ". Otherwise respond "‰∏ç"



## Question

-   Is understanding the same thing as being able to produce output in response to input?

- @searle1980 - this is the difference between strong and weak AI


## Back to nice safe words

-   we usually don't worry too much about what words like intelligence, understanding, etc really mean
-   for most purposes, understanding something, and doing that thing, pretty well overlap
-   AI, unfortunately, is an exception
-   big difference between producing output and understanding here

## Why does this matter?

-   Because the current conversation around AI does violence to our usual understanding of basic terms (like intelligence)
    -   We need to do a bit of re-interpreting...
    -   ...particularly because AI can do the input-output part *really* well
-   (side effect) The Chinese Room is an excellent way of understanding what's going on inside some of the current tech

## The tech

-   AI = big umbrella term
-   More specific terms:
    -   *Algorithms* = rule-based ways of producing sensible output
    -   *Expert systems* = more sophisticated expertise-based production of output
    -   *Machine learning* = umbrella term for non-expertise-based production of output
    -   *Large Language Models* = a massively-succesful sub-species of machine learning

## So what's an algorithm?

::: columns
::: {.column width="25%"}
![](src/images/altair.jpg) [(Packard 1979)](https://www.goodreads.com/book/show/191062.The_Third_Planet_from_Altair)
:::

::: {.column width="60%"}
-   Algorithm = rule (roughly)
    -   if something happens, do something
-   made from expert input and evidence
:::
:::

## An example algorithm

![](src/images/nice_136.png){fig-align="center"}

## Related expertise-based tools

"See also..." references in indexes, library catalogues, wikipedia

. . .

![](src/images/cue.png) . . .

-   [Brilliant 1996 Master's dissertation](https://files.eric.ed.gov/fulltext/ED401916.pdf) looking at the state of "see also..." referencing in Ohio's public libraries

## How about something more complicated?


::: columns
:::: {.column width="35%"}

![](src/images/monitor.png)
::::

:::: {.column width="60%"}
- one problem with algorithms: how to handle conflicting information?
-   An expert system - [MYCIN](https://en.wikipedia.org/wiki/Mycin) [@shortliffe1975]
    -   designed to identify bacterial infections and suitable Rx
    -   600 rules, supplied by experts
    -   asks users a series of clinical questions
    -   combines the answers using a (fairly simple) inference system
    -   able to manage some conflicting information - unlike simpler algorithms

::::

:::

## Machine learning

-   A next step: can we provide learning rules to a system, and let it figure out the details for itself?

. . .

![https://commons.wikimedia.org/wiki/File:Supervised_machine_learning_in_a_nutshell.svg](src/images/sml.png)

## This is supervised learning

-   supervision = labelled observations used for training and testing
-   Lots of health examples with promising results:
    -   diabetic retinopathy [@mookiah2013]
    -   ECG [@aziz2021]
    -   fractures, melanoma, ...

## A dataset downside

::: columns
::: {.column width="40%"}
![[Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)](src/images/mnist.png)
:::

::: {.column width="50%"}
Producing labelled datasets is hard:

-   generally must be very large
-   generally requires expert classification
-   must be done with great accuracy
    -   scale bar problem [@winkler2021]
-   so dataset labelling is wildly expensive and thankless
    -   Is there a way of doing something similar without spending trillions classifying everything in the world by hand?
:::
:::

## Unsupervised learning

![](src/images/google.png){fig-align="center"}

## Unsupervised learning

![](src/images/explain.png){fig-align="center"}

## Unsupervised learning

![](src/images/go√∂gle.png){fig-align="center"}

## Unsupervised learning

-   No-one is writing a list of possible searches starting with "Large..."
-   Nor are they classifying searches into likely/unlikely, then training a model
-   Instead, the model is looking at data (searches, language, location, trends) and calculating probabilities
    -   [2011 blog post](https://googleblog.blogspot.com/2011/04/more-predictions-in-autocomplete.html)
    -   [2020 PR piece](https://blog.google/products/search/how-google-autocomplete-predictions-work/)
    -   [2020 build your own in JS](https://medium.com/analytics-vidhya/build-a-simple-autocomplete-model-with-your-own-google-search-history-ead26b3b6bd4)
-   The terminology gets confusing again at this point:
    -   some describe this as *deep learning*
    -   better to call this a *language model*

## Large language models

What if we were more ambitious with the scope of our language model?

. . .

::: columns
::: {.column width="30%"}
![](src/images/transformer.png)
:::

::: {.column width="60%"}
-   Find masses of language data
    -   chatGPT uses basically the whole web before September 2021
-   Build a model capable of finding patterns in that data
    -   Attention model used in chatGPT [@vaswani2017]
-   Allow the model to calculate probabilities based on those patterns
    -   lots of work going on at present allowing models to improve in response to feedback etc
:::
:::

## Large language models

-   superb at generating appropriate text, code, images, music...
-   but production vs understanding
    -   e.g. hallucinations, phantom functions...
-   training is extremely computationally expensive
    -   questions about inequality and regulatory moating
        -   no-one but FAANG-sized companies can afford to do this
    - training is also surprisingly manual
    
    
## Public health strengths

* they're fantastic at writing style - so re-drafting public-facing information
* they're good for speeding-up repetitious production tasks (designing Power Automate flows)
* they can be used for wrangling free text, because that's basically how they work

## But!

+ IG involvement is **mandatory** - this is new and complex territory
+ you need a **human in the loop** - AI does not think and will misbehave unpredictably
+ they should not be used to **substitute for expertise** - don't try anything you wouldn't have the skills to attempt without an AI
+ don't trust them with **truthy** content - they'll get it weirdly wrong
+ don't let them **source** material for you - they'll make up the perfect reference...
+ don't rely on product **features** too heavily - they're changing fast

## Ethics

- your web content, my model, my paycheque
- where's the consent here?
- big serious worries about bias in some kinds of output
- rights violations via AI
- no settled questions around responsibility
- UK GDPR etc assume data is identifiable. That's not true in LLMs.

## Punchline

-   On balance, while there's hype here, there's also lots of substance and interest
-   LLMs have become *much* better at producing plausible output, across a *greatly* expanded area
-   A strength: fantastic ways for those with expertise to work faster
-   A danger: LLMs are great at producing truth-like output. Good enough so that some will be tempted to use them to extend their apparent expertise...
-   But big serious legal and ethical trouble ahead - we're not good at dealing with distributed responsibility

## Thumbs-up for specificity
+ many of the touted benefits are technology-specific
    + e.g. if we want to understand why decisions are getting made in a particular way, an expert system is better than a LLM
+ we should probably start asking "what do you mean by AI" whenever we're trying to make decisions about it

## Conclusion

1. The *intelligence* part of AI is as misleading as a swimming submarine
1. There are lots of different technologies that currently fall under the AI umbrella
1. points 1. and 2. really matter because of the hype

## Why the hype matters

- hype leads to perverse incentives and malfeasance: call any rubbish AI, and get paid for it
- that means that both what we mean by AI, and what tech gets included, is extra-important at present - there's an industry out there that's profiting from blurring the boundaries

## Feedback

[Feedback link](https://forms.office.com/Pages/ResponsePage.aspx?id=veDvEDCgykuAnLXmdF5Jmn79kl25VpJIq3eErXXCYKBUMlhWQVRTMERFVjU5TUdMSEVEMzYwODRWVC4u&r763e4a3a535149438ffa4d7812e07773=An%20introduction%20to%20AI%20Copilot&r96bb56f248fb457899d2a813d349450f=%222025-05-27%22)


Please give us one minute of your time. We add feedback comments to our training pages, because we think this is the most useful resource for people looking for specific training that suits their needs


